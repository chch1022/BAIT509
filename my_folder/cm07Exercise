Bagging is a special case of random forests under which case?

Bagging is a special case of random forests when m = p

What are the hyperparameters we can control for random forests?

n_estimators = number of trees in the foreset
max_features = max number of features considered for splitting a node
max_depth = max number of levels in each decision tree
min_samples_split = min number of data points placed in a node before the node is split
min_samples_leaf = min number of data points allowed in a leaf node
bootstrap = method for sampling data points (with or without replacement)


Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?
(1,0), (1,2), (1,5) - Not valid, (1,0) does not exist in the original data
(1,2), (2,0) - Valid
(1,2), (1,2), (1,5) - Valid 


For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)?
For (1,2), (2,0): (1,5) is out-of-bag
For (1,2),(1,2),(1,5): (2,0) is out-of-bag


You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases?
Regression: your trees make the following four predictions: 1,1,3,3. Prediction: 2 ( average of 1,1,3,3)
Classification: your trees make the following four predictions: "A", "A", "B", "C". Prediction: A (the most popular vote)
